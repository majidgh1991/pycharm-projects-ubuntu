from nltk.tokenize import sent_tokenize, word_tokenize

# def tokenize(text):
#
#     word_tokens = list()
#     for s in sent_tokenize(text):
#         word_tokens += word_tokenize(s)
#
#     return word_tokens

def tokenize_string(string):
        """
        I designed this method to be used independently of an obj/field. If this is the case, call _tokenize_field.
        It's more robust.
        :param string: e.g. 'salt lake city'
        :return: list of tokens
        """
        list_of_sentences = list()
        tmp = list()
        tmp.append(string)
        k = list()
        k.append(tmp)
        # print k
        list_of_sentences += k  # we are assuming this is a unicode/string

        word_tokens = list()
        for sentences in list_of_sentences:
            # print sentences
            for sentence in sentences:
                for s in sent_tokenize(sentence):
                    word_tokens += word_tokenize(s)

        return word_tokens

def preprocess_tokens(tokens_list, options=['lower']):
    """

    :param tokens_list: The list generated by tokenize_field per object
    :param options: A list of to-dos.
    :return: A list of processed tokens. The original list is unmodified.
    """
    new_list = list(tokens_list)
    for option in options:
        if option == 'remove_non_alpha':
            tmp_list = list()
            for token in new_list:
                if token.isalpha():
                    tmp_list.append(token)
            del new_list
            new_list = tmp_list
        elif option == 'lower':
            for i in range(0, len(new_list)):
                new_list[i] = new_list[i].lower()
        else:
            print 'Warning. Option not recognized: ' + option

    return new_list

def preprocess_text(text):
    return preprocess_tokens(tokenize_string(text))

# def text_to_lower(text):
#     return text.lower()
#
def list_to_lower(orig_list):
    return map(lambda x:x.lower(),orig_list)
#
# def tokens_remove_non_alpha(tokens):
#     tokens_less_non_alpha = list()
#     for token in tokens:
#         if token.isalpha():
#             tokens_less_non_alpha.append(token)
#     return tokens_less_non_alpha