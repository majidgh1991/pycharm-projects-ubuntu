import codecs
import json
from nltk.tokenize import sent_tokenize, word_tokenize
import re


class TextPreprocessors:
    """
    Contains static methods for taking the json objects and pre-processing/condensing text fields in them
    so they are more suitable for word-embedding code.
    """
    @staticmethod
    def _tokenize_field(obj, field):
        """
        At present, we'll deal with only one field (e.g. readability_text). The field could be a unicode
        or a list, so make sure to take both into account.

        We are not preprocessing the tokens in any way. For this, I'll write another function.
        :param obj: the adultservice json object
        :param field: e.g. 'readability_text'
        :return: A list of tokens.
        """
        list_of_sentences = list()

        #print obj['readability_text']
        if field not in obj:
            return None
        elif type(obj[field]) == list:
            k = list()
            k.append(obj[field])
            list_of_sentences += k
        else:
            tmp = list()
            tmp.append(obj[field])
            k = list()
            k.append(tmp)
            # print k
            list_of_sentences += k  # we are assuming this is a unicode/string

        word_tokens = list()
        for sentences in list_of_sentences:
            # print sentences
            for sentence in sentences:
                for s in sent_tokenize(sentence):
                    word_tokens += word_tokenize(s)

        return word_tokens

    @staticmethod
    def _preprocess_tokens(tokens_list, options=['remove_non_alpha','lower']):
        """

        :param tokens_list: The list generated by tokenize_field per object
        :param options: A list of to-dos.
        :return: A list of processed tokens. The original list is unmodified.
        """
        new_list = list(tokens_list)
        for option in options:
            if option == 'remove_non_alpha':
                tmp_list = list()
                for token in new_list:
                    if token.isalpha():
                        tmp_list.append(token)
                del new_list
                new_list = tmp_list
            elif option == 'lower':
                for i in range(0, len(new_list)):
                    new_list[i] = new_list[i].lower()
            else:
                print 'Warning. Option not recognized: '+option

        return new_list

    @staticmethod
    def _preprocess_sampled_annotated_file(sample_file, output_file):
        """
        We sampled files in FieldAnalyses.sample_n_values_from_field, and then labeled them. The problem is
        that we sampled raw values, and now I've done too much labeling to rectify. This is a one-time piece of
         code for the two files we have already sampled/labeled.
        :param sample_file:
        :return:
        """
        out = codecs.open(output_file, 'w', 'utf-8')
        with codecs.open(sample_file, 'r', 'utf-8') as f:
            for line in f:
                fields = re.split('\t',line)
                word_tokens = list()
                for s in sent_tokenize(fields[0]):
                    word_tokens += word_tokenize(s)
                fields[0] = ' '.join(word_tokens)
                out.write('\t'.join(fields))
        out.close()

    @staticmethod
    def build_tokens_objects_from_readability(input_file, output_file):
        """

        :param input_file: A json lines file
        :param output_file: A tokens file
        :return: None
        """
        field = 'readability_text'
        out = codecs.open(output_file, 'w', 'utf-8')
        with codecs.open(input_file, 'r', 'utf-8') as f:
            for line in f:
                tokens_obj = dict()
                obj = json.loads(line)
                tokenized_field = TextPreprocessors._tokenize_field(obj, field)
                if tokenized_field:
                    tokens_obj[obj['identifier']] = TextPreprocessors._preprocess_tokens(tokenized_field, options=["lower"])
                    json.dump(tokens_obj, out)
                    out.write('\n')
        out.close()

    @staticmethod
    def convert_txt_dict_to_json(input_file, output_file):
        results = list()
        with codecs.open(input_file, 'r', 'utf-8') as f:
            for line in f:
                results.append(line[0:-1])
        out = codecs.open(output_file, 'w', 'utf-8')
        json.dump(results, out, indent=4)
        out.close()

    @staticmethod
    def preprocess_annotated_cities_file(input_file, output_file):
        """
        We will take in a file such as annotated-cities-1.json as input and output another json that:
        tokenizes the high_recall_readability_text field and converts it to lower-case.
        converts values in the other two fields to lowercase

        These preprocessed files can then be used for analysis.

        Note that the field names remain the same in the output file, even though high_recall-* is now
         a list of tokens instead of a string.
        :param input_file:
        :param output_file:
        :return:
        """
        out = codecs.open(output_file, 'w', 'utf-8')
        with codecs.open(input_file, 'r', 'utf-8') as f:
            for line in f:
                obj = json.loads(line)
                tokenized_field = TextPreprocessors._tokenize_field(obj, 'high_recall_readability_text')
                if tokenized_field:
                    obj['high_recall_readability_text'] = TextPreprocessors._preprocess_tokens(tokenized_field, options=["lower"])
                    for k in obj.keys():
                        obj[k] = TextPreprocessors._preprocess_tokens(obj[k], options=["lower"])
                    json.dump(obj, out)
                    out.write('\n')
        out.close()



path='/home/mayankkejriwal/Downloads/memex-cp4-october/annotated-cities-experiments/'
TextPreprocessors.preprocess_annotated_cities_file(path+'raw-data/annotated-cities-2.json',
                                                path+'prepped-data/annotated-cities-2-prepped.json')
# TextPreprocessors.convert_txt_dict_to_json(path+'dictionaries/spa-massage-words.txt', path+'dictionaries/spa-massage-words.json')
# TextPreprocessors.build_tokens_objects_from_readability(path+'corpora/part-00000.json',
# path+'tokens/readability_tokens-large-corpus-onlyLower.json')


