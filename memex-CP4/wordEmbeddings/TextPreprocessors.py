import codecs
import json
from nltk.tokenize import sent_tokenize, word_tokenize
import re


class TextPreprocessors:
    """
    Contains static methods for taking the json objects and pre-processing/condensing text fields in them
    so they are more suitable for word-embedding code.
    """
    @staticmethod
    def _tokenize_field(obj, field):
        """
        At present, we'll deal with only one field (e.g. readability_text). The field could be a unicode
        or a list, so make sure to take both into account.

        We are not preprocessing the tokens in any way. For this, I'll write another function.
        :param obj: the adultservice json object
        :param field: e.g. 'readability_text'
        :return: A list of tokens.
        """
        list_of_sentences = list()

        #print obj['readability_text']
        if field not in obj:
            return None
        elif obj[field] is list:
            list_of_sentences += obj[field]
        else:
            list_of_sentences.append(obj[field])  # we are assuming this is a unicode/string

        word_tokens = list()
        for sentences in list_of_sentences:
            # print sentences
            for sentence in sentences:
                for s in sent_tokenize(sentence):
                    word_tokens += word_tokenize(s)

        return word_tokens

    @staticmethod
    def _preprocess_tokens(tokens_list, options=['remove_non_alpha','lower']):
        """

        :param tokens_list: The list generated by tokenize_field per object
        :param options: A list of to-dos.
        :return: A list of processed tokens. The original list is unmodified.
        """
        new_list = list(tokens_list)
        for option in options:
            if option == 'remove_non_alpha':
                tmp_list = list()
                for token in new_list:
                    if token.isalpha():
                        tmp_list.append(token)
                del new_list
                new_list = tmp_list
            elif option == 'lower':
                for i in range(0, len(new_list)):
                    new_list[i] = new_list[i].lower()
            else:
                print 'Warning. Option not recognized: '+option

        return new_list

    @staticmethod
    def _preprocess_sampled_annotated_file(sample_file, output_file):
        """
        We sampled files in FieldAnalyses.sample_n_values_from_field, and then labeled them. The problem is
        that we sampled raw values, and now I've done too much labeling to rectify. This is a one-time piece of
         code for the two files we have already sampled/labeled.
        :param sample_file:
        :return:
        """
        out = codecs.open(output_file, 'w', 'utf-8')
        with codecs.open(sample_file, 'r', 'utf-8') as f:
            for line in f:
                fields = re.split('\t',line)
                word_tokens = list()
                for s in sent_tokenize(fields[0]):
                    word_tokens += word_tokenize(s)
                fields[0] = ' '.join(word_tokens)
                out.write('\t'.join(fields))
        out.close()

    @staticmethod
    def build_tokens_objects_from_readability(input_file, output_file):
        """

        :param input_file: A json lines file
        :param output_file: A tokens file
        :return: None
        """
        field = 'readability_text'
        out = codecs.open(output_file, 'w', 'utf-8')
        with codecs.open(input_file, 'r', 'utf-8') as f:
            for line in f:
                tokens_obj = dict()
                obj = json.loads(line)
                tokenized_field = TextPreprocessors._tokenize_field(obj, field)
                if tokenized_field:
                    tokens_obj[obj['identifier']] = TextPreprocessors._preprocess_tokens(tokenized_field, options=["lower"])
                    json.dump(tokens_obj, out)
                    out.write('\n')
        out.close()

    @staticmethod
    def convert_txt_dict_to_json(input_file, output_file):
        results = list()
        with codecs.open(input_file, 'r', 'utf-8') as f:
            for line in f:
                results.append(line[0:-1])
        out = codecs.open(output_file, 'w', 'utf-8')
        json.dump(results, out, indent=4)
        out.close()

# path='/home/mayankkejriwal/Downloads/memex-cp4-october/'
# TextPreprocessors.convert_txt_dict_to_json(path+'dictionaries/spa-massage-words.txt', path+'dictionaries/spa-massage-words.json')
# TextPreprocessors.build_tokens_objects_from_readability(path+'corpora/part-00000.json',
# path+'tokens/readability_tokens-large-corpus-onlyLower.json')


