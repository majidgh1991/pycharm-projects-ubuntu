import codecs, gzip
import json
from nltk.tokenize import sent_tokenize, word_tokenize
import re
import pprint


class TextPreprocessors:
    """
    Contains static methods for taking the json objects and pre-processing/condensing text fields in them
    so they are more suitable for word-embedding code.
    """

    @staticmethod
    def is_sublist_in_big_list(big_list, sublist):
        # matches = []
        for i in range(len(big_list)):
            if big_list[i] == sublist[0] and big_list[i:i + len(sublist)] == sublist:
                return True
        return False

    @staticmethod
    def tokenize_string(string):
        """
        I designed this method to be used independently of an obj/field. If this is the case, call _tokenize_field.
        It's more robust.
        :param string: e.g. 'salt lake city'
        :return: list of tokens
        """
        list_of_sentences = list()
        tmp = list()
        tmp.append(string)
        k = list()
        k.append(tmp)
        # print k
        list_of_sentences += k  # we are assuming this is a unicode/string

        word_tokens = list()
        for sentences in list_of_sentences:
            # print sentences
            for sentence in sentences:
                for s in sent_tokenize(sentence):
                    word_tokens += word_tokenize(s)

        return word_tokens

    @staticmethod
    def _tokenize_field(obj, field):
        """
        At present, we'll deal with only one field (e.g. readability_text). The field could be a unicode
        or a list, so make sure to take both into account.

        We are not preprocessing the tokens in any way. For this, I'll write another function.
        :param obj: the adultservice json object
        :param field: e.g. 'readability_text'
        :return: A list of tokens.
        """
        list_of_sentences = list()

        #print obj['readability_text']
        if field not in obj:
            return None
        elif type(obj[field]) == list:
            k = list()
            k.append(obj[field])
            list_of_sentences += k
        else:
            tmp = list()
            tmp.append(obj[field])
            k = list()
            k.append(tmp)
            # print k
            list_of_sentences += k  # we are assuming this is a unicode/string

        word_tokens = list()
        for sentences in list_of_sentences:
            # print sentences
            for sentence in sentences:
                for s in sent_tokenize(sentence):
                    word_tokens += word_tokenize(s)

        return word_tokens

    @staticmethod
    def _preprocess_tokens(tokens_list, options=['lower']):
        """

        :param tokens_list: The list generated by tokenize_field per object
        :param options: A list of to-dos.
        :return: A list of processed tokens. The original list is unmodified.
        """
        new_list = list(tokens_list)
        for option in options:
            if option == 'remove_non_alpha':
                tmp_list = list()
                for token in new_list:
                    if token.isalpha():
                        tmp_list.append(token)
                del new_list
                new_list = tmp_list
            elif option == 'lower':
                for i in range(0, len(new_list)):
                    new_list[i] = new_list[i].lower()
            else:
                print 'Warning. Option not recognized: '+option

        return new_list

    @staticmethod
    def _preprocess_sampled_annotated_file(sample_file, output_file):
        """
        We sampled files in FieldAnalyses.sample_n_values_from_field, and then labeled them. The problem is
        that we sampled raw values, and now I've done too much labeling to rectify. This is a one-time piece of
         code for the two files we have already sampled/labeled.
        :param sample_file:
        :return:
        """
        out = codecs.open(output_file, 'w', 'utf-8')
        with codecs.open(sample_file, 'r', 'utf-8') as f:
            for line in f:
                fields = re.split('\t',line)
                word_tokens = list()
                for s in sent_tokenize(fields[0]):
                    word_tokens += word_tokenize(s)
                fields[0] = ' '.join(word_tokens)
                out.write('\t'.join(fields))
        out.close()

    @staticmethod
    def _extract_name_strings_from_dict_lists(obj, field='telephone', return_as_tokens = False):
        """
        We're assuming that obj contains 'field' (make sure to have checked for this) which is a list
        containing dicts. Each dict contains a name field. We will return a string of phone numbers in
        alphabetic order.
        :param obj:
        :param field: e.g. telephone or email
        :param return_as_tokens: if True we will return a list, otherwise we'll join and return as string.
        :return: A string, (sorted) list of unique tokens or None (if no names exist within the list)
        """
        phones = set()
        for phone in obj[field]:
            if 'name' in phone and phone['name']:
                if type(phone['name']) == list:
                    phones = phones.union(set(phone['name']))
                else:
                    phones.add(phone['name'])
        if not phones:
            return None
        else:
            phones = list(phones)
            phones.sort()
            if return_as_tokens:
                return phones
            else:
                return '-'.join(phones)

    @staticmethod
    def concat_bioinfo_tokens_objects(pos_id_file, neg_id_file, output_file):
        """
        I'm writing this file because I want to learn embeddings over the joint corpus and also distinguish
        between the two files (intact and mgi) without going to a lot of trouble. The neg id file will
        have its keys negated before writing out to output file.
        :param pos_id_file:
        :param neg_id_file:
        :param output_file:
        :return:
        """
        out = codecs.open(output_file, 'w', 'utf-8')
        with codecs.open(pos_id_file, 'r', 'utf-8') as f:
            for line in f:
                json.dump(json.loads(line), out)
                out.write('\n')
        with codecs.open(neg_id_file, 'r', 'utf-8') as f:
            for line in f:
                obj = json.loads(line)
                key = obj.keys()[0] # there should only be one key
                obj[-1*int(key)] = obj.values()[0]
                del obj[key]
                json.dump(obj, out)
                out.write('\n')
        out.close()

    @staticmethod
    def build_tokens_objects_for_persona_linking(string_converted_file, output_file):
        out = codecs.open(output_file, 'w', 'utf-8')
        with codecs.open(string_converted_file, 'r', 'utf-8') as f:
            for line in f:
                obj = json.loads(line)
                answer = dict()
                for k, v in obj.items():
                    answer[k] = TextPreprocessors._preprocess_tokens(TextPreprocessors.tokenize_string(v))
                json.dump(answer, out)
                out.write('\n')
        out.close()

    @staticmethod
    def build_tokens_objects_from_bioinfo_abstracts(input_file, output_file):
        """
        Designed for processing intact and mgi.jl files. At present only use abstracts. We'll use the line number
        in the jlines file starting from 1 as key, since I don't see an obvious id.
        :param input_file:
        :param output_file:
        :return:
        """
        out = codecs.open(output_file, 'w', 'utf-8')
        count = 1
        type_errors = 0
        key_errors = 0
        pp = pprint.PrettyPrinter(indent=4)
        with codecs.open(input_file, 'r', 'utf-8') as f:
            for line in f:
                # print 'in document...',
                # print count
                tmp = dict()
                obj = json.loads(line)
                try:
                        abstract_tokens = list()
                        # print type(obj['article']['front']['article-meta']['abstract']['p']['#text'])
                        if type(obj['article']['front']['article-meta']['abstract']['p'])==dict:
                            abstract_tokens = TextPreprocessors.tokenize_string(obj['article']['front']['article-meta']['abstract']['p']['#text'])
                        elif type(obj['article']['front']['article-meta']['abstract']['p']) == list:
                            for t in obj['article']['front']['article-meta']['abstract']['p']:
                                abstract_tokens += TextPreprocessors.tokenize_string(t)
                        else:
                            abstract_tokens = TextPreprocessors.tokenize_string(
                                obj['article']['front']['article-meta']['abstract']['p'])
                        key = count
                        tmp[key] = TextPreprocessors._preprocess_tokens(abstract_tokens, ['lower'])
                        json.dump(tmp, out)
                        out.write('\n')
                        count += 1
                except KeyError:
                    print 'key error in line: ',
                    # print line
                    count += 1
                    key_errors += 1
                    continue
                except TypeError:
                    print 'type error in line...',
                    print count
                    type_errors += 1
                    # print 'p-type is...',
                    # print type(obj['article']['front']['article-meta']['abstract']['p'])
                    count += 1
                    continue
        out.close()
        print 'processed lines...',
        print count
        print 'encountered type errors in...',
        print type_errors
        print 'encountered key errors in...',
        print key_errors

    @staticmethod
    def build_cluster_id_lists_for_CP1_summer(input_file, output_file):
        """
        Each cluster id will be a key in the json lines file output, and doc_id will be in the list of values
        that the key references
        :param input_file:  the original input file
        :param output_file:
        :return:
        """
        cluster_dict = dict()
        count = 1
        with codecs.open(input_file, 'r', 'utf-8') as f:
            for line in f:
                if count % 1000 == 0:
                    print count
                obj = json.loads(line)
                cluster_id = obj['cluster_id']
                doc_id = obj['doc_id']
                if cluster_id not in cluster_dict:
                    cluster_dict[cluster_id] = list()
                cluster_dict[cluster_id].append(doc_id)
                count += 1
        out = codecs.open(output_file, 'w', 'utf-8')
        for k, v in cluster_dict.items():
            answer = dict()
            answer[k] = v
            json.dump(answer, out)
            out.write('\n')
        out.close()

    @staticmethod
    def build_cluster_id_lists_for_CP1_november(input_file, output_file_pos, output_file_neg):
        """
        Each cluster id will be a key in the json lines file output, and _id will be in the list of values
        that the key references
        :param input_file:  the original input file
        :param output_file_pos:
        :return:
        """
        cluster_dict_pos = dict()
        cluster_dict_neg = dict()
        count = 1
        with codecs.open(input_file, 'r', 'utf-8') as f:
            for line in f:
                if count % 1000 == 0:
                    print count
                obj = json.loads(line)
                cluster_id = obj['cluster_id']
                doc_id = obj['_id']
                if obj['class'] == 1:
                    if cluster_id not in cluster_dict_pos:
                        cluster_dict_pos[cluster_id] = list()
                    cluster_dict_pos[cluster_id].append(doc_id)
                else:
                    if cluster_id not in cluster_dict_neg:
                        cluster_dict_neg[cluster_id] = list()
                    cluster_dict_neg[cluster_id].append(doc_id)
                count += 1

        out = codecs.open(output_file_pos, 'w', 'utf-8')
        for k, v in cluster_dict_pos.items():
            answer = dict()
            answer[k] = v
            json.dump(answer, out)
            out.write('\n')
        out.close()

        out = codecs.open(output_file_neg, 'w', 'utf-8')
        for k, v in cluster_dict_neg.items():
            answer = dict()
            answer[k] = v
            json.dump(answer, out)
            out.write('\n')
        out.close()

    @staticmethod
    def build_tokens_objects_from_CP1_summer(input_file, output_file):
        out = codecs.open(output_file, 'w', 'utf-8')
        count = 1
        not_found = 0
        found = 0
        with codecs.open(input_file, 'r', 'utf-8') as f:
            for line in f:
                obj = json.loads(line)
                tokens_obj = dict()
                if count%1000==0:
                    print count
                try:
                    if 'extracted_text' in obj:

                        tokens_obj[obj['doc_id']]=TextPreprocessors._preprocess_tokens(
                            TextPreprocessors.tokenize_string(obj['extracted_text']))
                        found += 1
                        json.dump(tokens_obj, out)
                        out.write('\n')
                        count += 1
                    else:
                        print 'No extracted text in doc...',
                        print obj['doc_id']
                        not_found += 1
                        count += 1
                except:
                    print 'some exception occurred...',
                    print line
                    not_found += 1
                    continue
        print 'objects processed successfully...',
        print found
        print 'objects processed unsuccessfully...',
        print not_found
        out.close()

    @staticmethod
    def build_tokens_objects_from_CP1_november(input_file, output_file_pos, output_file_neg):
        pos = codecs.open(output_file_pos, 'w', 'utf-8')
        neg = codecs.open(output_file_neg, 'w', 'utf-8')
        count = 1
        not_found = 0
        found = 0
        with codecs.open(input_file, 'r', 'utf-8') as f:
            for line in f:
                obj = json.loads(line)
                tokens_obj = dict()
                if count % 1000 == 0:
                    print count
                try:
                    if 'extracted_text' in obj:

                        tokens_obj[obj['_id']] = TextPreprocessors._preprocess_tokens(
                            TextPreprocessors.tokenize_string(obj['extracted_text']))
                        found += 1
                        if 'class' in obj:
                            if obj['class'] == 1:
                                json.dump(tokens_obj, pos)
                                pos.write('\n')
                            else:
                                json.dump(tokens_obj, neg)
                                neg.write('\n')
                        count += 1
                    else:
                        print 'No extracted text in doc...',
                        print obj['doc_id']
                        not_found += 1
                        count += 1
                except:
                    print 'some exception occurred...',
                    print line
                    not_found += 1
                    continue
        print 'objects processed successfully...',
        print found
        print 'objects processed unsuccessfully...',
        print not_found
        pos.close()
        neg.close()


    @staticmethod
    def build_tokens_objects_from_companies(input_file, output_file):
        out = codecs.open(output_file, 'w', 'utf-8')
        count = 1
        with codecs.open(input_file, 'r', 'utf-8') as f:
            for line in f:
                obj = json.loads(line)
                tokens_list = list()
                key = None
                if count%1000==0:
                    print count
                for k,v in obj.items():
                    key = k
                    for list_of_items in v:
                        tokens_list += TextPreprocessors._preprocess_tokens(list_of_items, ['lower'])
                answer = dict()
                answer[key] = tokens_list
                json.dump(answer, out)
                out.write('\n')
                count += 1
        out.close()


    @staticmethod
    def build_tokens_objects_from_march_CP1(input_files, output_file):
        out = codecs.open(output_file, 'w', 'utf-8')
        count = 1
        for input_file in input_files:
         with codecs.open(input_file, 'r', 'utf-8') as f:
            for line in f:
                obj = json.loads(line)
                tokens_list = list()
                cluster_id = obj['cluster_id']

                if count % 1000 == 0:
                    print count
                try:
                    doc_id = obj['_id']
                    tokens_list += TextPreprocessors._preprocess_tokens(
                        TextPreprocessors.tokenize_string(obj['_source']['extracted_text']), ['lower'])
                    answer = dict()
                    answer['tags'] = list()
                    answer['tags'].append(doc_id)
                    answer['tags'].append(cluster_id)
                    answer['words'] = tokens_list
                    json.dump(answer, out)
                    out.write('\n')
                    count += 1
                    # if 'annotation' in obj:
                    #     answer['annotation'] = obj['annotation']

                except:
                    print 'failure in object: ', str(obj['_id'])
                    # for k, v in obj.items():
                    #     key = k
                    #     for list_of_items in v:
                    #         tokens_list += TextPreprocessors._preprocess_tokens(list_of_items, ['lower'])

        out.close()


    @staticmethod
    def build_tokens_objects_from_microcap(input_file, output_file):
        out = codecs.open(output_file, 'w', 'utf-8')
        count = 1
        with codecs.open(input_file, 'r', 'utf-8') as f:
            for line in f:
                obj = json.loads(line)
                tokens_list = list()
                key = obj['_id']
                if count % 1000 == 0:
                    print count
                try:
                    tokens_list += TextPreprocessors._preprocess_tokens(
                        TextPreprocessors.tokenize_string(obj['extractors']['content_relaxed']
                                                          ['text']), ['lower', 'remove_non_alpha'])
                    answer = dict()
                    answer['tags'] = list()
                    answer['tags'].append(key)
                    answer['words']=tokens_list
                    json.dump(answer, out)
                    out.write('\n')
                    count += 1

                except:
                    print 'failure in object: ',str(obj['_id'])
                # for k, v in obj.items():
                #     key = k
                #     for list_of_items in v:
                #         tokens_list += TextPreprocessors._preprocess_tokens(list_of_items, ['lower'])

        out.close()

    @staticmethod
    def build_tokens_objects_from_microcap_dump_1(input_file, output_file):
        out = codecs.open(output_file, 'w', 'utf-8')
        count = 1
        with codecs.open(input_file, 'r', 'utf-8') as f:
            for line in f:
                outer_obj = json.loads(line)
                tokens_list = list()
                key = outer_obj[0]
                obj = outer_obj[1]
                if count % 1000 == 0:
                    print count
                try:
                    for text in obj['extractors']['content_relaxed']['text']:
                        tokens_list += TextPreprocessors._preprocess_tokens(
                        TextPreprocessors.tokenize_string(text['result']['value']), ['lower'])
                    if len(tokens_list) < 1:
                        continue
                    answer = dict()
                    answer['tags'] = list()
                    answer['tags'].append(key)
                    answer['words'] = tokens_list
                    json.dump(answer, out)
                    out.write('\n')
                    count += 1

                except:
                    print 'failure in object: ', str(outer_obj[0])
                    # for k, v in obj.items():
                    #     key = k
                    #     for list_of_items in v:
                    #         tokens_list += TextPreprocessors._preprocess_tokens(list_of_items, ['lower'])

    @staticmethod
    def build_tokens_objects_from_nyu_readability(gz_input_file, hrr_output_file, lrr_output_file):
        """

        :param gz_input_file:
        :param hrr_output_file: for high recall readability
        :param lrr_output_file: for low recall readability
        :return:
        """
        # field = 'readability_text'
        hrr_out = codecs.open(hrr_output_file, 'w', 'utf-8')
        lrr_out = codecs.open(lrr_output_file, 'w', 'utf-8')
        count = 1
        with gzip.open(gz_input_file, 'rb') as f:
            for line in f:
                hrr_tokens_obj = dict()
                lrr_tokens_obj = dict()
                obj = json.loads(line)
                if 'readability' in obj:
                    for item in obj['readability']:
                        # print item
                        # print item['recall_priority']
                        # print item['recall_priority'] == 'True'
                        if item['recall_priority'] == 'True':
                            # print 'got in hrr'
                            hrr_tokenized_field = TextPreprocessors.tokenize_string(item['result']['value'])
                            if hrr_tokenized_field:
                                    hrr_tokens_obj[obj['_id']] = TextPreprocessors._preprocess_tokens(hrr_tokenized_field,
                                                                                         options=["lower"])
                                    json.dump(hrr_tokens_obj, hrr_out)
                                    hrr_out.write('\n')
                        else:
                            # print 'got in lrr'
                            lrr_tokenized_field = TextPreprocessors.tokenize_string(item['result']['value'])
                            if lrr_tokenized_field:
                                lrr_tokens_obj[obj['_id']] = TextPreprocessors._preprocess_tokens(lrr_tokenized_field,
                                                                                                  options=["lower"])
                                json.dump(lrr_tokens_obj, lrr_out)
                                lrr_out.write('\n')
                if count%10000==0:
                    print 'processed document: ',
                    print count
                count += 1
                # break
        hrr_out.close()
        lrr_out.close()

    @staticmethod
    def build_tokens_objects_from_readability(input_file, output_file):
        """

        :param input_file: A json lines file
        :param output_file: A tokens file
        :return: None
        """
        field = 'readability_text'
        out = codecs.open(output_file, 'w', 'utf-8')
        with codecs.open(input_file, 'r', 'utf-8') as f:
            for line in f:
                tokens_obj = dict()
                obj = json.loads(line)
                tokenized_field = TextPreprocessors._tokenize_field(obj, field)
                if tokenized_field:
                    tokens_obj[obj['identifier']] = TextPreprocessors._preprocess_tokens(tokenized_field, options=["lower"])
                    json.dump(tokens_obj, out)
                    out.write('\n')
        out.close()

    @staticmethod
    def build_tokens_objects_from_nyu_data(input_file, output_file):
        """
        We only perform lower-case preprocessing for the moment.
        :param input_file: contains a single json object with keys (URIs) referencing long strings of scraped text.
        :param output_file: a tokens json lines file, with a key referencing a list of tokens.
        :return: None
        """
        inFile = codecs.open(input_file, 'r', 'utf-8')
        outFile = codecs.open(output_file, 'w', 'utf-8')
        obj = json.load(inFile)
        for k,v in obj.items():
            tmp = dict()
            tokenized_field = TextPreprocessors.tokenize_string(v)
            tmp[k] = TextPreprocessors._preprocess_tokens(tokenized_field, options=["lower"])
            json.dump(tmp, outFile)
            outFile.write('\n')
        inFile.close()
        outFile.close()

    @staticmethod
    def combine_jsons(input_files, output_file):
        """
        will raise an exception if keys clash across input files. Also, is memory intensive at present, will
        read in all jsons from the input_files before doing any writing. Thus, relative ordering may not be maintained.
        :param input_files: A list of files, with each file being  a json (not jlines) file
        :param output_file: An output file
        :return: None
        """
        big_dict = dict()
        for f in input_files:
            inFile = codecs.open(f, 'r', 'utf-8')
            TextPreprocessors.merge_dicts(big_dict, json.load(inFile))
            inFile.close()
        outFile = codecs.open(output_file, 'w', 'utf-8')
        json.dump(big_dict, outFile)
        outFile.close()

    @staticmethod
    def merge_dicts(dict_1, dict_2):
        """
        Merges dict_2 into dict_1 (hence, transforms dict_1). Only performs merging at upper level. Will raise exception if keys clash
        :param dict_1:
        :param dict_2:
        :return: None
        """
        for k, v in dict_2.items():
            if k in dict_1:
                raise Exception
            else:
                dict_1[k] = v


    @staticmethod
    def build_phone_objects_from_all_fields(input_file, output_file, exclude_fields = None, exclude_field_regex = None):
        """
        Be careful about the assumptions for the field structure. This function is not going to be appropriate for
        every jlines file.
        :param input_file: A json lines file
        :param output_file: A tokens file, where an identifier has two fields: tokens and phone. Both are lists. Be
        careful about usage; we will use this file primarily for generating phone embeddings.
        :param exclude_fields: If the field is within this list, we will ignore that field.
        :param exclude_field_regex: a regex string, where, if the field name matches this regex, we ignore it.
        :return: None
        """
        # field = 'readability_text'
        out = codecs.open(output_file, 'w', 'utf-8')
        with codecs.open(input_file, 'r', 'utf-8') as f:
            for line in f:
                obj = json.loads(line)

                # get phone string, if one exists
                if 'telephone' not in obj:
                    continue
                else:
                    phone = TextPreprocessors._extract_name_strings_from_dict_lists(obj)
                    if not phone:
                        continue

                # get tokens list, if one exists
                tokens_list = []
                for k in obj.keys():
                    if k == 'telephone':
                        continue
                    if exclude_fields:
                        if k in exclude_fields:
                            continue
                    if exclude_field_regex:
                        pat = re.split(exclude_field_regex, k)
                        print pat
                        if not (pat and len(pat) == 1 and pat[0] == k):
                            continue
                    # print k
                    if k == 'email':
                        tokenized_field = TextPreprocessors._extract_name_strings_from_dict_lists(obj, 'email', True)
                    else:
                        tokenized_field = TextPreprocessors._tokenize_field(obj, k)
                    if tokenized_field:
                        tokens = TextPreprocessors._preprocess_tokens(tokenized_field, options=["lower"])
                        if tokens:
                            tokens_list += tokens

                if not tokens_list:
                    continue

                # assuming we made it this far, we have everything we need
                inner_obj = dict()
                inner_obj['phone'] = phone
                inner_obj['tokens_list'] = tokens_list
                tokens_obj = dict()
                tokens_obj[obj['identifier']] = inner_obj
                json.dump(tokens_obj, out)
                out.write('\n')
        out.close()

    @staticmethod
    def convert_txt_dict_to_json(input_file, output_file):
        results = list()
        with codecs.open(input_file, 'r', 'utf-8') as f:
            for line in f:
                results.append(line[0:-1])
        out = codecs.open(output_file, 'w', 'utf-8')
        json.dump(results, out, indent=4)
        out.close()

    @staticmethod
    def preprocess_annotated_cities_file(input_file, output_file):
        """
        We will take in a file such as annotated-cities-1.json as input and output another json that:
        tokenizes the high_recall_readability_text field and converts it to lower-case.
        converts values in the other two fields to lowercase

        These preprocessed files can then be used for analysis.

        Note that the field names remain the same in the output file, even though high_recall-* is now
         a list of tokens instead of a string.
        :param input_file:
        :param output_file:
        :return:
        """
        out = codecs.open(output_file, 'w', 'utf-8')
        with codecs.open(input_file, 'r', 'utf-8') as f:
            for line in f:
                obj = json.loads(line)
                tokenized_field = TextPreprocessors._tokenize_field(obj, 'high_recall_readability_text')
                if tokenized_field:
                    obj['high_recall_readability_text'] = TextPreprocessors._preprocess_tokens(tokenized_field, options=["lower"])
                    for k in obj.keys():
                        obj[k] = TextPreprocessors._preprocess_tokens(obj[k], options=["lower"])
                    json.dump(obj, out)
                    out.write('\n')
        out.close()

    @staticmethod
    def preprocess_annotated_file(input_file, text_field, output_file):
        """
        We will take in a file such as annotated-cities-1.json as input and output another json that:
        tokenizes the text( e.g. high_recall_readability_text field) and converts it to lower-case.
        converts values in all other fields to lowercase

        These preprocessed files can then be used for analysis.

        Note that the field names remain the same in the output file, even though high_recall-* is now
         a list of tokens instead of a string.
        :param input_file:
        :param text_field:
        :param output_file:
        :return:
        """
        out = codecs.open(output_file, 'w', 'utf-8')
        with codecs.open(input_file, 'r', 'utf-8') as f:
            for line in f:
                obj = json.loads(line)
                tokenized_field = TextPreprocessors._tokenize_field(obj, text_field)
                if tokenized_field:
                    # obj[text_field] = TextPreprocessors._preprocess_tokens(tokenized_field, options=["lower"])
                    obj[text_field] = tokenized_field
                    for k in obj.keys():
                        if type(obj[k]) == list:
                            # obj[k] = TextPreprocessors._preprocess_tokens(obj[k], options=["lower"])
                            pass
                        else:
                            tokenized_k = TextPreprocessors._tokenize_field(obj, k)
                            obj[k] = tokenized_k
                            # obj[k] = TextPreprocessors._preprocess_tokens(tokenized_k, options=["lower"])
                    json.dump(obj, out)
                    out.write('\n')
        out.close()


    @staticmethod
    def check_microcap_tokens_for_duplicates(tokens_file1, tokens_file2):
        ids1 = set()
        # ids2 = set()
        count = 0
        with codecs.open(tokens_file1, 'r', 'utf-8') as f:
            for line in f:
                ids1 = ids1.union(set(json.loads(line)['tags']))
        with codecs.open(tokens_file2, 'r', 'utf-8') as f:
            for line in f:
                if json.loads(line)['tags'][0] in ids1:
                    count += 1
        print 'intersection of ids...',str(count)


    @staticmethod
    def preprocess_annotated_files(input_folder, input_files, text_field, output_folder, output_prefix='tokens-'):
        """
        We will take in a file such as annotated-cities-1.json as input and output another json that:
        tokenizes the text( e.g. high_recall_readability_text field) and converts it to lower-case.
        converts values in all other fields to lowercase

        These preprocessed files can then be used for analysis.

        Note that the field names remain the same in the output file, even though high_recall-* is now
         a list of tokens instead of a string.
         :param input_folder: the folder where the list of files are. Make sure to include the slash at the end.
        :param input_files: a list of files
        :param text_field:
        :param output_folder: output folder. Make sure to include the slash at the end.
        :param output_prefix: a prefix that will be attached to each input file and that becomes the output file name
        :return:
        """
        for f in input_files:
            input_file = input_folder+f
            output_file = output_folder+output_prefix+f
            TextPreprocessors.preprocess_annotated_file(input_file, text_field, output_file)


    @staticmethod
    def modify_tags(input_file, output_file):
        out = codecs.open(output_file, 'w','utf-8')
        with codecs.open(input_file, 'r', 'utf-8') as f:
            for line in f:
                obj = json.loads(line)
                tag = str(obj['tags'][0])
                obj['tags'] = list()
                obj['tags'].append(tag+'-folder')
                json.dump(obj, out)
                out.write('\n')
        out.close()


    @staticmethod
    def write_doc_annotation_set_in_march_CP1(training_file, output_file):
        doc_annotations = dict()
        with codecs.open(training_file, 'r', 'utf-8') as f:
            for line in f:
                obj = json.loads(line)
                if 'annotation' in obj:
                    if 'VERY_RELEVANT' == obj['annotation']:
                        doc_annotations[obj['_id']] = 1
                    elif 'NOT_RELEVANT' == obj['annotation']:
                        doc_annotations[obj['_id']] = 0
                    else:
                        raise Exception
        json.dump(doc_annotations, codecs.open(output_file, 'w', 'utf-8'))


    @staticmethod
    def write_cluster_annotation_set_in_march_CP1(training_file, output_file):
        cluster_annotations = dict()
        with codecs.open(training_file, 'r', 'utf-8') as f:
            for line in f:
                obj = json.loads(line)
                if 'annotation' in obj:
                    if 'VERY_RELEVANT' == obj['annotation']:
                        cluster_annotations[obj['cluster_id']] = 1
                    elif 'NOT_RELEVANT' == obj['annotation']:
                        cluster_annotations[obj['cluster_id']] = 0
                    else:
                        raise Exception
        json.dump(cluster_annotations, codecs.open(output_file, 'w', 'utf-8'))


    @staticmethod
    def write_test_cluster_annotation_set_in_march_CP1(test_file, output_file):
        cluster_annotations = dict()
        with codecs.open(test_file, 'r', 'utf-8') as f:
            for line in f:
                obj = json.loads(line)
                cluster_annotations[obj['cluster_id']] = 0


        json.dump(cluster_annotations, codecs.open(output_file, 'w', 'utf-8'))

    # @staticmethod
    # def append_annotation_to_tags(serialized_file, annotated_serialized_file):
    #     out = codecs.open(annotated_serialized_file, 'w', 'utf-8')
    #     with codecs.open(serialized_file, 'r', 'utf-8') as f:
    #         for line in f:
    #             obj = json.loads(line)
    #     out.close()


# www_path='/Users/mayankkejriwal/ubuntu-vm-stuff/home/mayankkejriwal/tmp/www-experiments/used-datasets/'
# path = '/Users/mayankkejriwal/Dropbox/dig-microcap-data/'
#path = '/Users/mayankkejriwal/Dropbox/memex-mar-17/CP1/'
#TextPreprocessors.write_test_cluster_annotation_set_in_march_CP1(path+'test_adjusted_unlabeled.json', path+'test_dummy_annotations.json')
# TextPreprocessors.append_annotation_to_tags(path+'serialized_train_test.jl', path+'serialized_annotated_train_test.jl')
# TextPreprocessors.build_tokens_objects_from_march_CP1([path+'train_adjusted.json',path+'test_adjusted_unlabeled.json'], path+'serialized_train_test.jl')
# TextPreprocessors.modify_tags(path+'all_docs_tokens_lower_non_alpha.jl',path+'all_docs_tokens_lower_non_alpha_mod.jl')
# input = path+'microcap_data_20170314'
# output = path+'microcap_lower'
# TextPreprocessors.build_tokens_objects_from_microcap_dump_1(input, output)
# names = ['globenewswire', 'hotstockedidx', 'hotstockednews', 'investopedia', 'microstockprofitblog',
#          'otcarchives', 'seekingalphastkideas']
# for name in names:
#     input_file = path+'mx_'+name+'/mx_'+name+'_readability.jl'
#     output_file = path+name+'_tokens_non_alpha.jl'
#     TextPreprocessors.build_tokens_objects_from_microcap(input_file, output_file)
# nyuTextPath='/Users/mayankkejriwal/datasets/memex-evaluation-november/nyu-text/'
# personaPath = '/Users/mayankkejriwal/datasets/memex-evaluation-november/persona-linking/'
# TextPreprocessors.build_tokens_objects_for_persona_linking(personaPath+'str-users-posts14.jl',personaPath+'tokens-14.jl')
# CP1Path = '/Users/mayankkejriwal/datasets/memex-evaluation-november/CP-1-summer/'
# TextPreprocessors.build_tokens_objects_from_CP1_november(CP1Path+'CP1_train_ads_labelled_fall2016.jsonl',
#                         CP1Path+'positive_tokens.json', CP1Path+'negative_tokens.json')
# TextPreprocessors.build_cluster_id_lists_for_CP1_summer(CP1Path+'cp1_negative_train_UPDATED.json',
#                                                 CP1Path + 'negative_clusters.jl')
# companiesTextPath = '/Users/mayankkejriwal/datasets/companies/'
# bioInfoPath = '/Users/mayankkejriwal/datasets/bioInfo/2016-11-08-intact_mgi_comparison/'
# TextPreprocessors.concat_bioinfo_tokens_objects(bioInfoPath+'mgi_tokens.jl', bioInfoPath+'intact_tokens.jl', bioInfoPath+'mgiPos_intactNeg_tokens.jl')
# TextPreprocessors.build_tokens_objects_from_bioinfo_abstracts(bioInfoPath+'intact.jl', bioInfoPath+'intact_tokens.jl')
# TextPreprocessors.build_tokens_objects_from_companies(companiesTextPath+'result.jl',companiesTextPath+'result-prepped.jl')
# TextPreprocessors.build_tokens_objects_from_nyu_readability(gz_input_file=nyuTextPath+'output2.gz',
#                         hrr_output_file=nyuTextPath+'hrr_output2.jl', lrr_output_file=nyuTextPath+'lrr_output2.jl')
# TextPreprocessors.build_tokens_objects_from_nyu_readability(gz_input_file=nyuTextPath+'output3.gz',
#                         hrr_output_file=nyuTextPath+'hrr_output3.jl', lrr_output_file=nyuTextPath+'lrr_output3.jl')
# TextPreprocessors.build_tokens_objects_from_nyu_readability(gz_input_file=nyuTextPath+'output4.gz',
#                         hrr_output_file=nyuTextPath+'hrr_output4.jl', lrr_output_file=nyuTextPath+'lrr_output4.jl')
# TextPreprocessors.build_tokens_objects_from_nyu_readability(gz_input_file=nyuTextPath+'output5.gz',
#                         hrr_output_file=nyuTextPath+'hrr_output5.jl', lrr_output_file=nyuTextPath+'lrr_output5.jl')
# TextPreprocessors.build_tokens_objects_from_nyu_readability(gz_input_file=nyuTextPath+'output6.gz',
#                         hrr_output_file=nyuTextPath+'hrr_output6.jl', lrr_output_file=nyuTextPath+'lrr_output6.jl')
# files = ['ann_city_title_state_1_25.json',
# 'ann_city_title_state_26_50.json',
# 'annotations_with_alt_1-25.json',
# 'annotations_with_alt_26-50.json',
# 'annotations_with_cities_gt_15000_1-25.json',
# 'annotations_with_cities_gt_15000_25-50.json',
# 'annotations_with_text_1-25.json',
# 'annotations_with_text_26-50.json']
# TextPreprocessors.preprocess_annotated_file(www_path+'raw/names.json', 'content', www_path+'tokens/tokens-names.json')
# TextPreprocessors.preprocess_annotated_files(www_path+'raw/', files, 'high_recall_readability_text', www_path+'tokens/')

# data_path = '/Users/mayankkejriwal/datasets/nyu_data/'
# TextPreprocessors.preprocess_annotated_cities_file(path+'raw-data/annotated-cities-2.json',
#                                                 path+'prepped-data/annotated-cities-2-prepped.json')
# TextPreprocessors.convert_txt_dict_to_json(path+'dictionaries/spa-massage-words.txt', path+'dictionaries/spa-massage-words.json')
# TextPreprocessors.combine_jsons([data_path+'pos_ht_data.json',data_path+'neg_ht_data.json'], data_path+'combined_ht_data.json')
# TextPreprocessors.build_tokens_objects_from_nyu_data(data_path+'combined_ht_data.json', data_path+'tokens_combined_ht_onlyLower.json')
# TextPreprocessors.build_tokens_objects_from_readability(path+'part-00000.json', path+'readability_tokens-part-00000-onlyLower.json')
# exclude_fields_1 = ['high_recall_readability_text', 'identifier', 'inferlink_text', 'readability_text', 'seller']
# exclude_field_regex = '\.*_count'
# string = 'readability_text'
# print re.split(exclude_field_regex, string)
# print '-'.join(exclude_fields_1)
# TextPreprocessors.build_phone_objects_from_all_fields(path+'part-00000.json',
# path+'all_tokens-part-00000-onlyLower-1.json', exclude_fields_1, exclude_field_regex)
# print TextPreprocessors.tokenize_string('salt')
