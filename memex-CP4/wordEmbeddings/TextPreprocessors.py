import codecs
import json
from nltk.tokenize import sent_tokenize, word_tokenize


class TextPreprocessors:
    """
    Contains static methods for taking the json objects and pre-processing/condensing text fields in them
    so they are more suitable for word-embedding code.
    """
    @staticmethod
    def _tokenize_field(obj, field):
        """
        At present, we'll deal with only one field (e.g. readability_text). The field could be a unicode
        or a list, so make sure to take both into account.

        We are not preprocessing the tokens in any way. For this, I'll write another function.
        :param obj: the adultservice json object
        :param field: e.g. 'readability_text'
        :return: A list of tokens.
        """
        list_of_sentences = list()
        list_of_tokens = list()
        #print obj['readability_text']
        if field not in obj:
            return None
        elif obj[field] is list:
            list_of_sentences += obj[field]
        else:
            list_of_sentences.append(obj[field])  # we are assuming this is a unicode/string

        word_tokens = list()
        for sentences in list_of_sentences:
            # print sentences
            for sentence in sentences:
                for s in sent_tokenize(sentence):
                    word_tokens += word_tokenize(s)

        return word_tokens

    @staticmethod
    def _preprocess_tokens(tokens_list, options=['remove_non_alpha','lower']):
        """

        :param tokens_list: The list generated by tokenize_field per object
        :param options: A list of to-dos.
        :return: A list of processed tokens. The original list is unmodified.
        """
        new_list = list(tokens_list)
        for option in options:
            if option == 'remove_non_alpha':
                tmp_list = list()
                for token in new_list:
                    if token.isalpha():
                        tmp_list.append(token)
                del new_list
                new_list = tmp_list
            elif option == 'lower':
                for i in range(0, len(new_list)):
                    new_list[i] = new_list[i].lower()
            else:
                print 'Warning. Option not recognized: '+option

        return new_list

    @staticmethod
    def build_tokens_objects_from_readability(input_file, output_file):
        """

        :param input_file: A json lines file
        :param output_file: A tokens file
        :return: None
        """
        field = 'readability_text'
        out = codecs.open(output_file, 'w', 'utf-8')
        with codecs.open(input_file, 'r', 'utf-8') as f:
            for line in f:
                tokens_obj = dict()
                obj = json.loads(line)
                tokenized_field = TextPreprocessors._tokenize_field(obj, field)
                if tokenized_field:
                    tokens_obj[obj['identifier']] = TextPreprocessors._preprocess_tokens(tokenized_field)
                    json.dump(tokens_obj, out)
                    out.write('\n')
        out.close()


# path='/home/mayankkejriwal/Downloads/memex-cp4-october/'
# TextPreprocessors.build_tokens_objects_from_readability(path+'part-00000.json', path+'readability_tokens-large-corpus.json')